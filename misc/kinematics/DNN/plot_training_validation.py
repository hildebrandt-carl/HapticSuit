import ast
import math
import glob

import numpy as np
import matplotlib.pyplot as plt

direction = "inverse"

# This is generated by running "test_dnn.py"
best_model = "20220906-115141-erjyb"

# Get all models
result_files = glob.glob("./results/{}/*.txt".format(direction))

training_accuracies     = []
training_losses         = []
validation_accuracies   = []
validation_losses       = []
models                  = []
batch_sizes             = []

# For each model
for file_name in result_files:
    # Display which model we are loading
    print("Reading: {}".format(file_name))

    # Opening file
    f = open(file_name, 'r')
    count = 0
    
    # Read each line in the file
    for line in f:
        # Read the data
        line = line.strip()
        if "Training Accuracy: " in line and not "Final" in line:
            training_acc = ast.literal_eval(line[line.find('['):])
        if "Training Loss: " in line and not "Final" in line:
            training_lss = ast.literal_eval(line[line.find('['):])
        if "Validation Accuracy: " in line and not "Final" in line:
            validati_acc = ast.literal_eval(line[line.find('['):])
        if "Validation Loss: " in line and not "Final" in line:
            validati_lss = ast.literal_eval(line[line.find('['):])
        if "Model: " in line:
            current_model = line[line.find(':')+2:]
        if "Batch Size: " in line:
            current_batch_size = line[line.find(':')+2:]
    # Closing files
    f.close()

    # Save the data
    training_accuracies.append(training_acc)
    training_losses.append(training_lss)
    validation_accuracies.append(validati_acc)
    validation_losses.append(validati_lss)
    models.append(current_model)
    batch_sizes.append(current_batch_size)

# Create the plots
total_plots = len(training_accuracies)
grid_size = int(math.floor(math.sqrt(total_plots)))


# Training Accuracy vs Validation Accuracy
fig1, axis1 = plt.subplots(grid_size, grid_size + 1, figsize = (12, 5))
# Go through the plots
total_plotted = 0
for i in range(grid_size):
    for j in range(grid_size + 1):
       
        # Make sure we only plot for data we have
        if total_plotted >=  total_plots:
            break

        title_string = ""

        if best_model in result_files[total_plotted]:
            title_string += "Best - "

        title_string += str(batch_sizes[total_plotted]) + ": " + str(models[total_plotted])
        axis1[i, j].set_title(title_string)

        axis1[i, j].plot(training_accuracies[total_plotted])
        axis1[i, j].plot(validation_accuracies[total_plotted])
        # axis1[i, j].set_ylim([0.85, 1.05])
        axis1[i, j].set_ylim([0.301, 0.401])

        # Increment how many we plotted
        total_plotted += 1

fig1.suptitle('Training vs Validation Accuracy')
# fig1.tight_layout()





# Training Accuracy vs Validation Accuracy
fig2, axis2 = plt.subplots(grid_size, grid_size + 1, figsize = (12, 5))
# Go through the plots
total_plotted = 0
for i in range(grid_size):
    for j in range(grid_size + 1):
       
        # Make sure we only plot for data we have
        if total_plotted >=  total_plots:
            break

        title_string = ""

        if best_model in result_files[total_plotted]:
            title_string += "Best - "

        title_string += str(batch_sizes[total_plotted]) + ": " + str(models[total_plotted])
        axis2[i, j].set_title(title_string)

        axis2[i, j].plot(training_losses[total_plotted])
        axis2[i, j].plot(validation_losses[total_plotted])
        # axis2[i, j].set_ylim([0, 1])
        axis2[i, j].set_ylim([4.5, 6])

        # Increment how many we plotted
        total_plotted += 1

fig2.suptitle('Training vs Validation Loss')
# fig2.tight_layout()

# Show all plots
plt.show()
